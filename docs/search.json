[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Haddi",
    "section": "",
    "text": "Hi! My name is Haddi and I am a senior in college! I created this page for my intro to data science class. I enjoy hiking, dancing, and hanging out with my friends."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "shades.html",
    "href": "shades.html",
    "title": "Makeup Shades",
    "section": "",
    "text": "Foundation Shade Names\n\nThis data was provided by Amber Thomas to produce an article in collaboration with Pudding written by Ofunne Amaka. The article talks about how the names of shades in complexion products reveal bias. The code written by Amber Thomas uses the Sephora and Ulta websites to gather information about various shade names of complexion products from various companies. In order to analyze how similar shades of the same names were to each other, I used the tidy tuesday data that can be found here.\n\n\nCode\n# loading necessary packages and data\nlibrary(tidyverse)\nshades &lt;-read.csv(\"shades data/allShades.csv\")\n\n\n\n\nCode\necho = FALSE\n\n# beginning data wrangling\n# get the most frequent shade names\nspecific_shades &lt;- shades |&gt;\n  group_by(name) |&gt;\n  filter(!is.na(name)) |&gt;\n  summarize(freq = n()) |&gt;\n  filter(freq &gt; 60) |&gt;\n  distinct()\n\n#get the colors for the shades\nhexes &lt;- shades |&gt;\n  filter(name == \"Deep\" | name == \"Fair\" |  name == \"Light\" |\n        name == \"Medium\" | name == \"Neutral\" | name == \"Warm\") |&gt;\n  arrange(hex)\n\nhex_v &lt;- as.list(hexes[[\"hex\"]])\n\n\n\n\nCode\nshades |&gt;\n  filter(name == \"Deep\" | name == \"Fair\" |  name == \"Light\" |\n        name == \"Medium\" | name == \"Neutral\" | name == \"Warm\") |&gt;\n  arrange(hex)|&gt;\n  ggplot(aes(x = hue, y = lightness, color = hex)) +\n    geom_point() +\n    labs(\n      x = \"Hue\",\n      y = \"Lightness\",\n      title = \"Hue and Lightness of Makeup Shades with Same Shade Name\", \n      color = \"Name of Shade\"\n    ) +\n    theme_classic() +\n    guides(color = \"none\") +\n    scale_color_manual(values = hex_v) +\n    facet_wrap(~name)\n\n\n\n\n\nI used hue and lightness to compare the shade matches between foundation shades with the same name. The lightness and hue are clustered pretty well for shades like light and fair but get worse as you approach medium and are not very good for the deep shades. While shades named after undertones like neutral and warm have poor clusters. This validates a lot of complaints by women about inconsistency in shade names across different brands."
  },
  {
    "objectID": "phd.html",
    "href": "phd.html",
    "title": "PHD’s Awarded in the US",
    "section": "",
    "text": "PhD’s Awarded in the US\n\nThis data set was acquired from the National Center of Science and Engineering Statistics. Kelly Kang, a project officer compiled demographic information, educational history, sources of financial support, and post graduation plans of doctoral recipients. The link to this original data can be found here. Then I used a tidy data set from github. Using this data set, I chose to analyze which broad fields were awarding the most PhDs over the years.\n\n\nCode\n# loading necessary packages\nlibrary(tidyverse)\n\n\n\n\nCode\n# beginning data wrangling\n\n#load csv files into data frame that I can work with\nphds &lt;- read.csv(\"phd_by_field.csv\")\n\nphds |&gt;\n  group_by(broad_field, year) |&gt;\n  filter(!is.na(n_phds)) |&gt;\n  summarize(tot_phds = sum(n_phds), year) |&gt;\n  distinct() |&gt;\n  ggplot(aes(x = year, y = tot_phds, color = broad_field)) +\n    geom_smooth(se = FALSE) +\n    theme_classic() +\n    labs(\n      x = \"Year\",\n      y = \"PHD's Awarded\",\n      title = \"PHD's Awarded in Various Fields\",\n      color = \"Broad Field of PHD\"\n    )\n\n\n\n\n\nLife sciences awards the most PhDs by a healthy margin and it looks like the amount being awarded is still on an increasing trend. Engineering has the least PhDs awarded per year by broad field but it looks like the amount of engineering PhDs being awarded is increasing. Education PhD’s were on a decline but it looks like they are beginning to level off around 5000 PhDs awarded every year. More psychology and social sciences PhDs are being awarded each year."
  },
  {
    "objectID": "characanalysis.html",
    "href": "characanalysis.html",
    "title": "Analyzing Friends Dialogue",
    "section": "",
    "text": "Analyzing Friends Dialogue\n\nThis data was obtained from user EmilHvitfeldt on Github. You can find the link here. I will use the data to analyze how much they mention dating over the years of the show being aired and how viewership varies based on the name of the main character in the title of the episode.\n\n\nCode\n# load packages\nlibrary(tidyverse)\n\n\n\n\nCode\n# load data sets \nload(file=\"friendsdata/friends.rda\")\nload(file=\"friendsdata/friends_info.rda\")\n\n\n\n\nCode\n# how many times is the word date, dating, or dated is said per episode depending on the year\n\nbasic_info &lt;- friends_info |&gt;\n  mutate(year_aired = str_sub(air_date, 1,4))|&gt;\n  group_by(year_aired)|&gt;\n  mutate(numepyear = n()) |&gt;\n  select(season, episode, year_aired, numepyear)\n\nfriend_date &lt;- friends |&gt;\n  left_join(basic_info, by = c(\"season\", \"episode\")) |&gt;\n  group_by(year_aired) |&gt;\n  filter(str_detect(text, \"(?i)\\\\bdate|dating\\\\b\") == TRUE) |&gt;\n  summarise(date_ment = n(), numepyear)  |&gt;\n  distinct() |&gt;\n  mutate(datepeppyear = date_ment/numepyear)\n\nggplot(friend_date, aes(x = year_aired, y = datepeppyear)) +\n  geom_col() +\n  theme_bw() +\n  labs(\n    x = \"Year Aired\",\n    y = \"Number of times date said per episode\",\n    title = \"Concept of Dating Mentioned Over the Years in Friends Dialogue\"\n  )\n\n\n\n\n\nIn 1995,1996, 1998, and 1999 the word date was said a lot less often in friends. I have never watched friends but I assume that during these years, the main characters were in more long term relationships. This is probably why we see a dip in 2004 as well because they are wrapping up the show and giving the characters happy endings with a partner. Again this paragraph is written by someone who has never watched friends.\n\n\nCode\n# in the titles how common is it for a main character to get mentioned\n\nmain &lt;- friends_info |&gt;\n  filter(str_detect(title,\"The One\") == TRUE) |&gt;\n  mutate(title = str_replace(title, \"The One \", \"The One:\")) |&gt;\n  mutate(spec_title = str_extract(title, \"(?&lt;=:).+\"))|&gt;\n  mutate(main_char = if_else(str_detect(spec_title, \n                                \"(?i)\\\\bchandler|monica|joey|phoebe|rachel|ross\\\\b\"), str_extract(spec_title, \n                                \"(?i)\\\\bchandler|monica|joey|phoebe|rachel|ross\\\\b\"), \"None\"))\n  \n\nggplot(main, aes(fill = main_char, x = main_char, y = us_views_millions)) +\n  geom_boxplot() +\n  theme_bw() +\n  labs(x = \"Main Character in Title\",\n       y = \"Episode Views in millions\",\n       title = \"Friends Episode Viewership Based on Main Character in Title\") \n\n\n\n\n\nHaving some characters names in the title leads to a higher mean of viewers like Monica and Chandler but having Ross’ name in the title leads to a lower mean of viewers. Having the other main characters names in the tile leads to a similar mean value as having no main character name in the title at all. Overall there is a lot of overlap and the differences are not that significant. If I had a show and I wanted to know if putting certain characters names in the titles lead to more views than I would appreciate that knowledge."
  },
  {
    "objectID": "mdphdsim.html",
    "href": "mdphdsim.html",
    "title": "Simulation Study",
    "section": "",
    "text": "The simulation I am running will be about how many schools a medical school applicant is likely to get into. First I am using the data from the 2024 matriculant data from AAMC into US Medical Schools, the data set can be found here. I will be using this to have a list of schools with their state location and the acceptance rate of each school. The first part of the simulation will pick 20 random schools because most applicants usually apply to that many. Then it will pick a number between 0 and 100 if the number is lower than the acceptance rate than it will count how it as them getting into the school.Then I will run a secon similation to see how it differs if someone is from a region and would like to stay in that region. When running this simulation, I made it so that the number was between 0 and 50 because of in-state or same region preferences that some schools have.\n\n\nCode\nlibrary(tidyverse)\nlibrary(openxlsx)\n\n\n\n\nCode\n# open up the data and calculate acceptance rates\nmedschool &lt;- read.xlsx(\"mdsimdata.xlsx\", sheet = \"Usable\") |&gt;\n  mutate(acceptrate = matriculants/applicants, \n         instate_fact = instatem/instatea)\n\nnortheast &lt;-(\"CT|DC|MA|MD|NH|NJ|NY|PA|RI|VT\")\nsouth &lt;- (\"AL|AR|FL|GA|KY|KS|LA|NC|NE|OK|SC|TN|TX|VA|WV\")\nwest &lt;- (\"AZ|CA|CO|HI|NM|NV|OR|UT|WA|WY\")\nmidwest &lt;- (\"IA|IL|IN|MI|MO|MN|ND|OH|SD|WI\")\n\nreg_medschool &lt;- medschool |&gt;\n  mutate(state = str_sub(state, 1,2)) |&gt;\n  mutate(region = ifelse(str_detect(state, midwest) , \"Midwest\",\n           ifelse(str_detect(state, south), \"South\", \n                  ifelse(str_detect(state, west) , \"West\", \"Northeast\"\n             ))))\n\n\n\n\nCode\naccepted &lt;- function(numschools){\n  gen_qual = runif(1)\n  medschool |&gt;\n    sample_n(numschools) |&gt;\n    mutate(qual = rnorm(numschools, mean = gen_qual, sd = 0.1)) |&gt;\n    mutate(gotin = ifelse(qual &lt; acceptrate, 1, 0)) |&gt;\n    summarize(numaccept = sum(gotin))\n}\n\n\n\n\nCode\ntrials &lt;- rep(c(20), 500)\n\nsim &lt;- map_df(trials, accepted)|&gt;\n  rbind() \n\nggplot(sim) +\n  geom_histogram(aes(x = numaccept)) +\n  labs(\n    x = \"Number of Schools Accepted\",\n    y = \"Number of Applicants\",\n    title = \"Amount of Medical Schools Accepted Into When Applying to 20 Schools\"\n  )\n\n\n\n\n\nThis histogram shows that you have to be really lucky to get into multiple medical schools and it simulates that most people do not get into medical school.\n\n\nCode\nreg_opt &lt;-c(\"Midwest\", \"South\", \"West\", \"Northeast\")\n\naccepted_by_region &lt;- function(numschools){\n  reg_select &lt;- sample.int(4, 1)\n  reg_medschool |&gt;\n    filter(region == reg_opt[reg_select]) |&gt;\n    sample_n(size = numschools) |&gt;\n    mutate(qual = runif(numschools, min = 0, max = 0.5)) |&gt;\n    mutate(gotin = ifelse(qual &lt; acceptrate, 1, 0)) |&gt;\n    group_by(region) |&gt;\n    summarize(numaccept = sum(gotin))\n}\n\n\n\n\nCode\ntrials &lt;- rep(c(20), 500)\n\nsim &lt;- map_df(trials, accepted_by_region)|&gt;\n  rbind() \n\nggplot(sim) +\n  geom_histogram(aes(x = numaccept, fill = region)) +\n  labs(\n    x = \"Number of Schools Accepted\",\n    y = \"Number of Applicants\",\n    title = \"Amount of Medical Schools Accepted Into When Applying to 20 Schools\"\n  ) +\n  facet_wrap(~region)\n\n\n\n\n\nThis histogram makes it seem like schools in the Northeast are more likely to accept people than any other region. It may be helpful to see these graphs represented in proportions to have a more firm stance on how medical school acceptance may differ based on region. This also incorporates a big assumption that being a local to a region makes you a more attractive candidate. Some things I would do to improve my simulation in the future is create a function with more variables. The schools would have public or private so then I could do an if else for region preference.\n\nReferences\n\nAssociation of American Medical Colleges. Facts: Applicants and Matriculants Data. AAMC, https://www.aamc.org/data-reports/students-residents/data/facts-applicants-and-matriculants."
  },
  {
    "objectID": "ethics.html",
    "href": "ethics.html",
    "title": "Ethics",
    "section": "",
    "text": "Target and Pregnancy Predictions\n\nA 2012 article in the New York Times tells the story of how Target created a pregnancy predictor. Andrew Pole, a statistician began working at Target in 2002. Two colleagues in the marketing department asked him if he could predict if a customer was pregnant even if she did not want them to know. They went on this journey of creating an algorithm to predict pregnancy because of the fact that new parents are a holy grail for retailers. Most people remain consistent in their shopping habits and it is hard to change them as a retailer. The prime time to change people’s shopping habits is when they go through a big change in life, becoming parents is one of those changes. Wanting to seize the opportunity to gain new customers or change the shopping habits of infrequent customers, Target developed this algorithm and used it to advertise ads for baby items to pregnant women. This algorithm was allegly so specific it could estimate the month that the baby is due based on a woman’s change in shopping habits. This algorithm tracks 25 items that when analyzed together could give rise to a pregnancy score. An example of some of these items are unscented lotion, a large purse that could double as a diaper bag, and zinc and magnesium supplements.\nSource\nAn article by the Medium in 2020 dissects this article by the New York Times and addresses how the article might make the algorithm seem more advanced than it actually is. The original New York Times article, emphasizes a story where a father finds out that his teenage daughter was pregnant through the ads. The Medium article argues that the odds of Andrew Pole finding out that this happened in a random target is low and there is no proof that the target algorithm predicted that this specific girl was pregnant as opposed to her getting a random ad that happened to have some maternity things. The article then states that even if the story is true, it does not tell us about how good the algorithm itself is. The article uses the New York Times article as an example of how not to report an algorithm. This article advocates for the reporting of accuracy, false positive rate, false negative rate, precision, and recall of these algorithms so people can really gain a sense of how effective these algorithms are.\nSource\nHowever, neither of these articles address the fact that there is no consent from customers to do this. Even in the original article with an interview with Andrew Pole, his colleagues in the marketing department state they would like to know that the customer is pregnant even if she does not want them to know she was pregnant. On their website, they probably have the little cookies warning that tells them that they are tracking their information or if they become a member of a rewards program the customers are not aware that they are using the shopping history in order to predict whether they are pregnant or not. This sort of algorithm exists outside of the ordinary person’s imagination of what a company could do with their data.\nOne good thing about the data is that it was not publicly made available. Target has specific customer ids for each shopper and it is possible that if this data was publicly made available it could be used in other companies to predict pregnancy or it could reveal private information about a lot of people.\nThe people who were measured were customers at Target. These individuals were the intended audience of target for who the algorithm was intended for. There is no indication that they were informed that their data would be used for this specific algorithm. If this data was publicly made available it should not be analyzed because there is no indication that the customers were willing participants in the study to create this specific algorithm.\nThe observations for the algorithm are based on 25 items and how these were collected began with finding customers registered for a baby registry. Pole then found that women who were registered were buying unscented lotion early in their pregnancies. Then another analyst noticed that pregnant women were buying zinc and magnesium supplements sometime in the first 20 weeks. This practice of using the baby registry shows that participants in the study did not know or give their permission for their data to be used to create an invasive algorithm like this. Informed consent could have been possible by creating a survey for pregnant customers or customers that have previously been pregnant asking what products helped them while pregnant or what products they bought a lot of when pregnant. They could have compared the top results to the customer profiles of all the pregnant people who filled out the survey."
  },
  {
    "objectID": "sopp.html",
    "href": "sopp.html",
    "title": "Stanford Open Policing Project",
    "section": "",
    "text": "Stanford Open Policing Project\n\nIn this project, I used data compiled by Pierson et. al in 2020. The data and more about the Stanford Open Policing Project can be found on their website (&lt;a href=“https://openpolicing.stanford.edu”&gt;Click Here&lt;/a&gt;).\n\n\nCode\nlibrary(RMariaDB)\nlibrary(tidyverse)\n\n\n\n\nCode\ncon_traffic &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"traffic\",\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n)\n\nDBI::dbListTables(con_traffic)\n\n\n [1] \"ar_little_rock_2020_04_01\"      \"az_gilbert_2020_04_01\"         \n [3] \"az_mesa_2023_01_26\"             \"az_statewide_2020_04_01\"       \n [5] \"ca_anaheim_2020_04_01\"          \"ca_bakersfield_2020_04_01\"     \n [7] \"ca_long_beach_2020_04_01\"       \"ca_los_angeles_2020_04_01\"     \n [9] \"ca_oakland_2020_04_01\"          \"ca_san_bernardino_2020_04_01\"  \n[11] \"ca_san_diego_2020_04_01\"        \"ca_san_francisco_2020_04_01\"   \n[13] \"ca_san_jose_2020_04_01\"         \"ca_santa_ana_2020_04_01\"       \n[15] \"ca_statewide_2023_01_26\"        \"ca_stockton_2020_04_01\"        \n[17] \"co_aurora_2023_01_26\"           \"co_denver_2020_04_01\"          \n[19] \"co_statewide_2020_04_01\"        \"ct_hartford_2020_04_01\"        \n[21] \"ct_statewide_2020_04_01\"        \"fl_saint_petersburg_2020_04_01\"\n[23] \"fl_statewide_2020_04_01\"        \"fl_tampa_2020_04_01\"           \n[25] \"ga_statewide_2020_04_01\"        \"ia_statewide_2020_04_01\"       \n[27] \"id_idaho_falls_2020_04_01\"      \"il_chicago_2023_01_26\"         \n[29] \"il_statewide_2020_04_01\"        \"in_fort_wayne_2020_04_01\"      \n[31] \"ks_wichita_2023_01_26\"          \"ky_louisville_2023_01_26\"      \n[33] \"ky_owensboro_2020_04_01\"        \"la_new_orleans_2020_04_01\"     \n[35] \"ma_statewide_2020_04_01\"        \"md_baltimore_2020_04_01\"       \n[37] \"md_statewide_2020_04_01\"        \"mi_statewide_2020_04_01\"       \n[39] \"mn_saint_paul_2020_04_01\"       \"mo_statewide_2020_04_01\"       \n[41] \"ms_statewide_2020_04_01\"        \"mt_statewide_2023_01_26\"       \n[43] \"nc_charlotte_2020_04_01\"        \"nc_durham_2020_04_01\"          \n[45] \"nc_fayetteville_2020_04_01\"     \"nc_greensboro_2020_04_01\"      \n[47] \"nc_raleigh_2020_04_01\"          \"nc_statewide_2020_04_01\"       \n[49] \"nc_winston_salem_2020_04_01\"    \"nd_grand_forks_2020_04_01\"     \n[51] \"nd_statewide_2020_04_01\"        \"ne_statewide_2020_04_01\"       \n[53] \"nh_statewide_2020_04_01\"        \"nj_camden_2020_04_01\"          \n[55] \"nj_statewide_2020_04_01\"        \"nv_henderson_2020_04_01\"       \n[57] \"nv_statewide_2020_04_01\"        \"ny_albany_2020_04_01\"          \n[59] \"ny_statewide_2020_04_01\"        \"oh_cincinnati_2020_04_01\"      \n[61] \"oh_columbus_2020_04_01\"         \"oh_statewide_2020_04_01\"       \n[63] \"ok_oklahoma_city_2023_01_26\"    \"ok_tulsa_2020_04_01\"           \n[65] \"or_statewide_2020_04_01\"        \"pa_philadelphia_2020_04_01\"    \n[67] \"ri_statewide_2020_04_01\"        \"sc_statewide_2020_04_01\"       \n[69] \"sd_statewide_2020_04_01\"        \"tn_nashville_2020_04_01\"       \n[71] \"tn_statewide_2020_04_01\"        \"tx_arlington_2020_04_01\"       \n[73] \"tx_austin_2020_04_01\"           \"tx_garland_2020_04_01\"         \n[75] \"tx_houston_2023_01_26\"          \"tx_lubbock_2020_04_01\"         \n[77] \"tx_plano_2020_04_01\"            \"tx_san_antonio_2023_01_26\"     \n[79] \"tx_statewide_2020_04_01\"        \"va_statewide_2020_04_01\"       \n[81] \"vt_burlington_2023_01_26\"       \"vt_statewide_2020_04_01\"       \n[83] \"wa_seattle_2020_04_01\"          \"wa_statewide_2020_04_01\"       \n[85] \"wa_tacoma_2020_04_01\"           \"wi_madison_2023_01_26\"         \n[87] \"wi_statewide_2020_04_01\"        \"wy_statewide_2020_04_01\"       \n\n\n\n\nCode\nSELECT subject_sex,\n  subject_race,\n  AVG(warning_issued) AS perc_warning\nFROM wa_statewide_2020_04_01\nGROUP BY subject_race, subject_sex\nHAVING subject_sex IS NOT NULL AND subject_race IS NOT NULL\nORDER BY subject_race DESC;\n\n\n\n\nCode\nrace_warning_citation |&gt;\n  ggplot(aes(x = subject_race, y = perc_warning)) +\n    geom_col(aes(fill = subject_race)) +\n    labs(\n      x = \"Race of Subject\",\n      y = \"Percentage of Warnings\",\n      title = \"Percentage of Warnings in Washington State Traffic Stops by Race and Sex\"\n    ) +\n    facet_grid(~subject_sex)\n\n\n\n\n\nThis graph represents the proportion of traffic stops that end in warnings by race and gender in Washington State. Hispanic women seem to be slightly more likely than any other demographic to get a warning. Asian men and women are less likely to be let off with a warning than the other demographics. I am interested in how much negative stereotypes contribute to this result.\n\n\nCode\n\nSELECT subject_sex,\n  SUM(1) AS total,\n  'SF' AS city,\n  SUBSTRING(time, 1, 2) AS hour\nFROM ca_san_francisco_2020_04_01\nWHERE time IS NOT NULL\nGROUP BY subject_sex, hour\n\nUNION\n\nSELECT subject_sex,\n  SUM(1) AS total,\n  'LA' AS city,\n  SUBSTRING(time, 1, 2) AS hour\nFROM ca_los_angeles_2020_04_01\nWHERE time IS NOT NULL\nGROUP BY subject_sex, hour;\n\n\n\n\nCode\nca_sex_stops |&gt;\n  mutate(hour = as.numeric(hour)) |&gt;\n  group_by(city) |&gt;\n  mutate(total_stops = sum(total)) |&gt;\n  ungroup()|&gt;\n  mutate(prop_stops = total/total_stops) |&gt;\n  ggplot(aes(x = hour, y = prop_stops, color = subject_sex)) +\n    geom_line() +\n    labs(\n      x = \"Hour of the Day\",\n      y = \"Proportion of Stops\",\n      Title = \"Proportion of Stops in San Francisco vs California at Various Times of the Day\"\n    ) +\n    theme_classic()+\n    facet_grid(~city)\n\n\n\n\n\nThis graph represents proportion of stops that happen throughout different times of the day in San Francisco and LA separated by gender. One very obvious trend is that men are more likely to be pulled over. The least amount of traffic stops happen around 5 AM. In LA the peak time for traffic stops is 8 PM but in San Francisco the peak time for traffic stops is arounf 11 PM.\n\nReferences\n\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. \"A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.\" Nature Human Behaviour, 1–10."
  },
  {
    "objectID": "presentation.html#medical-school-acceptance-rate-simulation",
    "href": "presentation.html#medical-school-acceptance-rate-simulation",
    "title": "Medical School Acceptance Simulation",
    "section": "Medical School Acceptance Rate Simulation",
    "text": "Medical School Acceptance Rate Simulation\nBy Haddi Sise"
  },
  {
    "objectID": "presentation.html#the-data",
    "href": "presentation.html#the-data",
    "title": "Medical School Acceptance Simulation",
    "section": "The Data",
    "text": "The Data\n\nFrom the AAMC website, I took a data table titled “U.S. MD-Granting Medical School Applications and Matriculants by School, State of Legal Residence, and Gender, Academic Year 2024-2025”"
  },
  {
    "objectID": "presentation.html#the-approach-to-creating-the-simulation",
    "href": "presentation.html#the-approach-to-creating-the-simulation",
    "title": "Medical School Acceptance Simulation",
    "section": "The Approach to Creating the Simulation",
    "text": "The Approach to Creating the Simulation\n\nFrom matriculants and applications calculate acceptance rate for each school\nCreate a function in which an “applicant” selects twenty random schools\nRepeat the function 500 times\nGraph the number of applicants vs the number of schools accepted"
  },
  {
    "objectID": "presentation.html#tidying-the-data",
    "href": "presentation.html#tidying-the-data",
    "title": "Medical School Acceptance Simulation",
    "section": "Tidying The Data",
    "text": "Tidying The Data\n\nlibrary(tidyverse)\nlibrary(openxlsx)\n\n# open up the data and calculate acceptance rates\nmedschool &lt;- read.xlsx(\"mdsimdata.xlsx\", sheet = \"Usable\") |&gt;\n  mutate(acceptrate = matriculants/applicants)\n\nnortheast &lt;-(\"CT|DC|MA|MD|NH|NJ|NY|PA|RI|VT\")\nsouth &lt;- (\"AL|AR|FL|GA|KY|KS|LA|NC|NE|OK|SC|TN|TX|VA|WV\")\nwest &lt;- (\"AZ|CA|CO|HI|NM|NV|OR|UT|WA|WY\")\nmidwest &lt;- (\"IA|IL|IN|MI|MO|MN|ND|OH|SD|WI\")\n\nreg_medschool &lt;- medschool |&gt;\n  mutate(state = str_sub(state, 1,2)) |&gt;\n  mutate(region = ifelse(str_detect(state, midwest) , \"Midwest\",\n           ifelse(str_detect(state, south), \"South\", \n                  ifelse(str_detect(state, west) , \"West\", \"Northeast\"\n             ))))"
  },
  {
    "objectID": "presentation.html#the-original-function",
    "href": "presentation.html#the-original-function",
    "title": "Medical School Acceptance Simulation",
    "section": "The Original Function",
    "text": "The Original Function\n\naccepted &lt;- function(numschools){\n  medschool |&gt;\n    sample_n(numschools) |&gt;\n    mutate(qual = runif(numschools)) |&gt;\n    mutate(gotin = ifelse(qual &lt; acceptrate, 1, 0)) |&gt;\n    summarize(numaccept = sum(gotin))\n}"
  },
  {
    "objectID": "presentation.html#results",
    "href": "presentation.html#results",
    "title": "Medical School Acceptance Simulation",
    "section": "Results",
    "text": "Results\n\ntrials &lt;- rep(c(20), 500)\n\nsim &lt;- map_df(trials, accepted)|&gt;\n  rbind()\n\nggplot(sim) +\n  geom_histogram(aes(x = numaccept)) +\n  labs(\n    x = \"Number of Schools Accepted\",\n    y = \"Number of Applicants\",\n    title = \"Amount of Medical Schools Accepted Into When Applying to 20 Schools\"\n  ) +\n  theme_classic()"
  },
  {
    "objectID": "presentation.html#the-updated-function",
    "href": "presentation.html#the-updated-function",
    "title": "Medical School Acceptance Simulation",
    "section": "The Updated Function",
    "text": "The Updated Function\n\naccepted &lt;- function(numschools){\n  gen_qual = runif(1)\n  medschool |&gt;\n    sample_n(numschools) |&gt;\n    mutate(qual = rnorm(numschools, mean = gen_qual, sd = 0.1)) |&gt;\n    mutate(gotin = ifelse(qual &lt; acceptrate, 1, 0)) |&gt;\n    summarize(numaccept = sum(gotin))\n}"
  },
  {
    "objectID": "presentation.html#results-1",
    "href": "presentation.html#results-1",
    "title": "Medical School Acceptance Simulation",
    "section": "Results",
    "text": "Results\n\ntrials &lt;- rep(c(20), 500)\n\nsim &lt;- map_df(trials, accepted)|&gt;\n  rbind() \n\nggplot(sim) +\n  geom_histogram(aes(x = numaccept)) +\n  labs(\n    x = \"Number of Schools Accepted\",\n    y = \"Number of Applicants\",\n    title = \"Amount of Medical Schools Accepted Into When Applying to 20 Schools\"\n  ) +\n  theme_classic()"
  },
  {
    "objectID": "presentation.html#future-directions",
    "href": "presentation.html#future-directions",
    "title": "Medical School Acceptance Simulation",
    "section": "Future Directions",
    "text": "Future Directions\n\nSeeing how gender might affect the simulation\nIntroducing In-State Bias"
  },
  {
    "objectID": "presentation.html#thanks",
    "href": "presentation.html#thanks",
    "title": "Medical School Acceptance Simulation",
    "section": "Thanks!!",
    "text": "Thanks!!"
  },
  {
    "objectID": "presentation.html#sec-thanks",
    "href": "presentation.html#sec-thanks",
    "title": "Medical School Acceptance Simulation",
    "section": "Thanks!!",
    "text": "Thanks!!"
  }
]